{
    "collab_server" : "",
    "contents" : "# This is my attempt to replicate my DataCamp solution but on my local machine\n\n# Import the training set: train\ntrain_url <- \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/train.csv\"\ntrain <- read.csv(train_url, header = TRUE)\n\n# Import the testing set: test\ntest_url <- \"http://s3.amazonaws.com/assets.datacamp.com/course/Kaggle/test.csv\"\ntest <- read.csv(test_url, header = TRUE)\n\n# Print train and test to the console\ntrain\ntest\n\nstr(train)\nstr(test)\n\n# Survival rates in absolute numbers\ntable(train$Survived)\n\n# Survival rates in proportions\nprop.table(table(train$Survived))\n\n# Two-way comparison: Sex and Survived\ntable(train$Sex, train$Survived)\n\n# Two-way comparison: row-wise proportions\nprop.table(table(train$Sex,train$Survived), margin = 1)\n\n# Create the column child, and indicate whether child or no child\ntrain$Child <- NA\ntrain$Child[train$Age < 18] <- 1\ntrain$Child[train$Age >= 18] <- 0\n\n# Two-way comparison\nprop.table(table(train$Child,train$Survived),1)\n\n# Copy of test\ntest_one <- test\n\n# Initialize a Survived column to 0\ntest_one$Survived <- 0\n\n# Set Survived to 1 if Sex equals \"female\" (this is basically the AllMenDie model...)\ntest_one$Survived[test_one$Sex==\"female\"] <- 1\n\n# Load in the R package\nlibrary(\"rpart\")\n\n# Build the decision tree\nmy_tree_two <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked, data = train, method = \"class\")\n\n# Visualize the decision tree using plot() and text()\nplot(my_tree_two)\ntext(my_tree_two)\n\n# Load in the packages to build a fancy plot\n#install.packages('rattle')\nlibrary(rattle)\n#install.packages('rpart.plot')\nlibrary(\"rpart.plot\")\n#install.packages('RColorBrewer')\nlibrary(\"RColorBrewer\")\n\n\n# Time to plot your fancy tree\nfancyRpartPlot(my_tree_two)\n\n# Make predictions on the test set\nmy_prediction <- predict(my_tree_two, newdata = test, type = \"class\")\n\n# Finish the data.frame() call\nmy_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)\n\n# Use nrow() on my_solution\nnrow(my_solution)\n\n# Finish the write.csv() call\nwrite.csv(my_solution, file = \"my_solution.csv\", row.names = FALSE)\n\n# make the tree bigger\n\nmy_tree_three <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked,data = train, method = \"class\", control = rpart.control(minsplit = 50, cp = 0))\n\n# Visualize my_tree_three\nfancyRpartPlot(my_tree_three)\n\n# Create train_two\ntrain_two <- train\ntrain_two$family_size <- train_two$SibSp + train_two$Parch + 1\n\n# Finish the command\nmy_tree_four <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size, data = train_two, method = \"class\")\n\n# Visualize your new decision tree\nfancyRpartPlot(my_tree_four)\n\n\n\ntest$Survived <- NA\n# drop the Child column before merging train and test\ntrain <- train[,-13]\nall_data <- rbind(train, test)\n\nall_data$Embarked[c(62,830)] = \"S\"\nall_data$Embarked <- factor(all_data$Embarked)\n\nall_data$Fare[1044] <- median(all_data$Fare, na.rm = TRUE)\n\n# Create Title variable by stripping from Name field\nall_data$Name <- as.character(all_data$Name)\nall_data$Title <- sapply(all_data$Name, FUN=function(x) {strsplit(x, split='[,.]')[[1]][2]})\nall_data$Title <- sub(' ', '', all_data$Title)\nall_data$Title[all_data$Title %in% c('Mme', 'Mlle')] <- 'Mlle'\nall_data$Title[all_data$Title %in% c('Capt', 'Don', 'Major', 'Sir')] <- 'Sir'\nall_data$Title[all_data$Title %in% c('Dona', 'Lady', 'the Countess', 'Jonkheer')] <- 'Lady'\nall_data$Title <- factor(all_data$Title)\n\n# Create Deck variable from Cabin\n# Cabin has a lot of missings. Let's set those to S for Steerage.\nall_data$Cabin <- as.character(all_data$Cabin)\nall_data$Cabin[all_data$Cabin==\"\"] <- \"S\"\nall_data$Cabin[is.na(all_data$Cabin)] <- \"S\"\n# Cabins seem to have a leading value of either \"A\" \"B\" \"C\" \"D\" \"E\" \"F\" \"G\" \"S\" (that's the one I added), or \"T\".  Let's strip that out into a new variable.  I'm not sure if that is really corresponding to the deck of the ship, but I'll call it Deck anyway.\nall_data$Deck <- sapply(all_data$Cabin, FUN=function(x) {strsplit(x, split=\"\")[[1]][1]})\nall_data$Deck <- factor(all_data$Deck)\n\n# Create Family Size variable by adding siblings and parent/child\nall_data$family_size <- all_data$SibSp + all_data$Parch +1\n\n# Deal with missing Ages\npredicted_age <- rpart(Age ~ Pclass + Sex + SibSp + Parch + Fare + Embarked + Title + family_size, data=all_data[!is.na(all_data$Age),], method=\"anova\")\nall_data$Age[is.na(all_data$Age)] <- predict(predicted_age, all_data[is.na(all_data$Age),])\n\n# Split back into Train and Test\ntrain <- all_data[1:891,]\ntest <- all_data[892:1309,]\n\nset.seed(111) \n\n# Apply the Random Forest Algorithm\n\n#install.packages('randomForest')\nlibrary(\"randomForest\")\n\nmy_forest <- randomForest(as.factor(Survived) ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size + Title, data=train, importance = TRUE, ntree=1000) \n\nmy_prediction <- predict(my_forest, test, \"class\") \n\nmy_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction) \n\nwrite.csv(my_solution,file=\"my_solution.csv\" , row.names=FALSE) \n\nvarImpPlot(my_forest)\n\n# Now time to investigate if Cabin can improve the model...\n# Now try the best Decision Tree I had so far but with Deck as well.\nmy_tree_five <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size + Deck, data = train, method = \"class\")\nfancyRpartPlot(my_tree_five)\nmy_prediction <- predict(my_tree_five, test, type = \"class\")\nmy_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)\nwrite.csv(my_solution, file = \"my_solution.csv\", row.names = FALSE)\n# performance on Kaggle dropped.  Now I'm going to try controlling the tree more.\nmy_tree_five <- rpart(Survived ~ Pclass + Sex + Age + SibSp + Parch + Fare + Embarked + family_size + Deck, data = train, method = \"class\",control = rpart.control(minsplit = 50, cp = 0))\nfancyRpartPlot(my_tree_five)\nmy_prediction <- predict(my_tree_five, test, type = \"class\")\nmy_solution <- data.frame(PassengerId = test$PassengerId, Survived = my_prediction)\nwrite.csv(my_solution, file = \"my_solution.csv\", row.names = FALSE)\n\n# my own utility to convert the factor back to its proper numeric value (instead of the label of the level!)\nas.numeric.factor <- function(x) {as.numeric(levels(x))[x]}\n\n# Now try writing a function to calculate the accuracy rate on the train dataframe\ncalc.accuracy <- function(myprediction, the_answers) {\n  mistakes <- abs(myprediction-the_answers)\n  accuracy <- (length(myprediction)-sum(mistakes))/length(myprediction)\n  return(accuracy)\n}\n\n# calculate accuracy rate of my_tree_five\nmy_trainprediction <- predict(my_tree_five, train, type = \"class\")\nmy_testprediction <- predict(my_tree_five, test, type = \"class\")\nmy_scoredtrain <- data.frame(Survived = train$Survived, Score = my_trainprediction)\ncalc.accuracy(my_scoredtrain$Survived, as.numeric.factor(my_scoredtrain$Score))\n# [1] 0.8271605\n\n# calculate accuracy rate of my_tree_four\nmy_trainprediction <- predict(my_tree_four, train, type = \"class\")\nmy_testprediction <- predict(my_tree_four, test, type = \"class\")\nmy_scoredtrain <- data.frame(Survived = train$Survived, Score = my_trainprediction)\ncalc.accuracy(my_scoredtrain$Survived, as.numeric.factor(my_scoredtrain$Score))\n# [1] 0.8395062\n\n# create helper function to random sample rows from a dataframe\nrandomRows = function(df,n){\n  return(df[sample(nrow(df),n),])\n}\n\n# Now improve the function to calculate the accuracy rate on bootstrap samples from train\ncalc.bootstrap.accuracy <- function(myfile,m,n){\n  # myfile should have first column with your prediction and second column with the answer key\n  # pull m bootstrap samples of size n\n  list.of.results <- vector(mode = \"numeric\", length = m)\n  for (i in 1:m){\n    y <- randomRows(myfile,n)\n    list.of.results[i] <- calc.accuracy(y[,1], y[,2])\n  }\n  print(paste(\"Iteration results \",list.of.results))\n  bootstrap_accuracy <- mean(list.of.results)\n  print(paste(\"The bootstrap accuracy (mean) is \", bootstrap_accuracy))\n  return(bootstrap_accuracy)\n}\n\n# calculate bootstrap accuracy rate of my_tree_five\nmy_trainprediction <- predict(my_tree_five, train, type = \"class\")\nmy_testprediction <- predict(my_tree_five, test, type = \"class\")\nmy_scoredtrain <- data.frame(Score = as.numeric.factor(my_trainprediction), Survived = as.numeric(train$Survived))\ncalc.bootstrap.accuracy(my_scoredtrain,10,300)\n# [1] 0.819\n\n# calculate bootstrap accuracy rate of my_tree_four\nmy_trainprediction <- predict(my_tree_four, train, type = \"class\")\nmy_testprediction <- predict(my_tree_four, test, type = \"class\")\nmy_scoredtrain <- data.frame(Score = as.numeric.factor(my_trainprediction), Survived = as.numeric(train$Survived))\ncalc.bootstrap.accuracy(my_scoredtrain,10,300)\n# [1] 0.8416667\n\n# calculate bootstrap accuracy rate of my_forest (which should be wildly overfit...)\nmy_trainprediction <- predict(my_forest, train, type = \"class\")\nmy_testprediction <- predict(my_forest, test, type = \"class\")\nmy_scoredtrain <- data.frame(Score = as.numeric.factor(my_trainprediction), Survived = as.numeric(train$Survived))\ncalc.bootstrap.accuracy(my_scoredtrain,10,300)\n# [1] 0.9386667 Crap!  I thought for sure this performance would be lower.  I guess though that if it is accurate on train then it is accurate on subsets of train...",
    "created" : 1479231881226.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "528135630",
    "id" : "5FC37D73",
    "lastKnownWriteTime" : 1479340103,
    "last_content_update" : 1479340103232,
    "path" : "~/Documents/KaggleTitanic/KaggleTitanic_DataCampReplication.R",
    "project_path" : "KaggleTitanic_DataCampReplication.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}